{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dbd014-c64f-45cc-b9c1-1aa6ab88f40c",
   "metadata": {},
   "source": [
    "# Step 4: Retriever+Generator構築"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39399532-8557-4b4a-ab0b-66041743f551",
   "metadata": {},
   "source": [
    "本ステップでは、ここまでに経験してきた類似検索と準備したテンプレートを活用して、RetrieverとGeneratorを実装してする過程を経験します。\n",
    "- 類似検索ができるように、ベクトルデータベースへ接続して類似検索の検索オプションを指定してRetrieverを生成します\n",
    "- 投げかけられたクエリを基にRetrieverが類似検索を実行します\n",
    "- GeneratorがHugging FaceからOpen LLMを取得して読み込みます\n",
    "- Generatorが類似検索で得られた関連情報とテンプレートでプロンプトを生成し、LLMへ回答案の作成を依頼します\n",
    "![Step4](../image/rag-overview-step4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15150dd8-cc49-46ee-987c-ffe7640e0380",
   "metadata": {},
   "source": [
    "## 0. 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6714ad-527a-4cc0-ba64-a18bb2082626",
   "metadata": {},
   "source": [
    "### 共通処理/定数定義\n",
    "全ステップで共通して使用する定数とバナークラスを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f962899-340f-42fa-aa06-350ea3efdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib import myconstant\n",
    "from mylib.MyBanner import MyBanner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae19758-4cfb-4449-9de6-c0a0380381e2",
   "metadata": {},
   "source": [
    "### Access Token入力\n",
    "Open LLMの取得に必要となるHugging FaceのAccess Tokenを入力します。（事前に発行して入手しておきます）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444b56a-4558-47bb-8692-56c44c51522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from getpass import getpass\n",
    "HF_ACCESS_TOKEN = getpass(\"Hugging Face の Access Token を入力して Enter Key を押してください: \")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700a707-bb4c-4a83-8c6c-4c55ca3d4ab0",
   "metadata": {},
   "source": [
    "### パッケージインストール\n",
    "本ステップの処理で依存するパッケージをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2e30f-b77d-4990-8dc0-658b38ea68d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "!python -V\n",
    "!pip install langchain\n",
    "!pip install langchain-huggingface\n",
    "!pip install langchain_milvus\n",
    "!pip install accelerate\n",
    "\n",
    "!pip install ipywidgets\n",
    "!pip install urllib3==1.26.20\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd831bd9-4245-46cd-86ff-44927432429e",
   "metadata": {},
   "source": [
    "### import\n",
    "本ステップの処理で依存するモジュールを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f041f-66b9-47e3-bdaa-f66028b003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from mylib.MyEmbedding import MyEmbedding\n",
    "from mylib.MyMilvus import MyMilvus\n",
    "from mylib.MyOpenLlmList import MyOpenLlmList\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354f91f-000f-4eb9-9cb9-afdf0f755e98",
   "metadata": {},
   "source": [
    "## 1. 生成: ①.Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6286c-a70e-4422-bd94-c69b591db78b",
   "metadata": {},
   "source": [
    "### 【準備】Embedding Model読込\n",
    "Embedding Modelをメモリに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ef4ba-fbd1-4533-8779-32f65c9809b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "embeddings = MyEmbedding.get_model()\n",
    "print(f\"{embeddings=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0831051-e667-4a2a-be33-6231e814a121",
   "metadata": {},
   "source": [
    "### 【準備】Vector DB接続\n",
    "エンベディングで利用するEmbedding Modelと接続情報を渡してベクトルデータベースへ接続します。\n",
    "- 接続失敗した場合は、Milvus(milvus-standalone コンテナ)が起動しているか確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f17bfc-7ded-427a-868d-a4db9cc285ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "# connect to VectorDB\n",
    "vector_db = MyMilvus(\n",
    "    myconstant.VDB_HOST, myconstant.VDB_PORT,\n",
    "    myconstant.VDB_USER, myconstant.VDB_PASS, embeddings)\n",
    "print(f\"{vector_db=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e4c96-e530-4ba3-b09b-2f4ec6ae0889",
   "metadata": {},
   "source": [
    "### 【準備】Doc Store接続\n",
    "RDBのテーブルに該当するドキュメントストアに接続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7da333-7b98-44d5-9382-99c1f0ecc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "# connect to a store in Vector DB\n",
    "docstore_list = vector_db.get_collections()\n",
    "docstore_name = docstore_list[0]\n",
    "docstore = vector_db.connect(docstore_name)\n",
    "print(f\"{docstore_list=}\")\n",
    "print(f\"{docstore_name=}\")\n",
    "print(f\"{docstore=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606ee67-5331-425b-bc07-4b24dd640140",
   "metadata": {},
   "source": [
    "### 【生成】Retriever Object\n",
    "類似検索の実行に必要なRetrieverオプジェクトを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ad4ed-3a4c-4cf6-8cbe-e76555ad6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "# RAG向けのVDB Retriever生成\n",
    "retriever = vector_db.get_retriever(docstore)\n",
    "print(f\"{retriever=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441189a3-401f-48cb-aed0-d6cc2846c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "# 事前確認\n",
    "query = \"パソコンの使い方を学べるセミナーを教えてください\"\n",
    "my_docs = retriever.invoke(query)\n",
    "print(\"* \" + \"\\n---------\\n* \".join(doc.page_content for doc in my_docs))\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ad0b9-4352-44e8-9b8f-5ccc84eb64b6",
   "metadata": {},
   "source": [
    "## 2. 生成: ②.Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218d5b8-6f29-485d-a757-bc79e790b4f5",
   "metadata": {},
   "source": [
    "### 【定義】Generator Class\n",
    "以下の役割を担うGeneratorクラスを定義してクラスファイルに書き出します。\n",
    "- 一覧で指定されたOpen LLMをHugging Faceから読み込みメモリで管理する\n",
    "- クエリを基にして類似検索の実行する\n",
    "- 類似検索で得られた関連情報とテンプレートを用いたプロンプトを生成する\n",
    "- 生成したプロンプトでOpen LLMへ回答案作成依頼を連携するchainを生成する\n",
    "\n",
    "実装に使っているtransformersモジュールの仕様は、Hugging Faceから以下を参照してください。\n",
    "-  https://huggingface.co/docs/transformers/v4.46.3/ja/model_doc/auto#transformers.AutoTokenizer\n",
    "-  https://huggingface.co/docs/transformers/ja/model_doc/auto#transformers.AutoTokenizer.from_pretrained\n",
    "-  https://huggingface.co/docs/transformers/ja/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a532e6fa-62c4-4a74-8782-3ebc6830dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyGenerator.py\n",
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, pipeline)\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import inspect\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from mylib.MyTemplateImpl4Gemma import MyTemplateImpl4Gemma\n",
    "from mylib.MyTemplateImpl4MsPhi import MyTemplateImpl4MsPhi\n",
    "from mylib.MyTemplateImpl4OpenCalm import MyTemplateImpl4OpenCalm\n",
    "from mylib.MyTemplateImpl4RinnaJpGpt import MyTemplateImpl4RinnaJpGpt\n",
    "from mylib.MyTemplateImpl4Llama3 import MyTemplateImpl4Llama3\n",
    "from mylib.MyTemplateImpl4DeepSeek import MyTemplateImpl4DeepSeek\n",
    "\n",
    "class MyGenerator:\n",
    "\n",
    "    def __init__(self, model_name_list, access_token):\n",
    "        my_method = inspect.currentframe().f_code.co_name\n",
    "        prompts = {}\n",
    "        for index, model_name in enumerate(model_name_list):\n",
    "            print(\"\\nStart @ [%s] >>> index=%d: [%s]\" % (my_method, index, model_name))\n",
    "            prompts[model_name] = {}\n",
    "            prompts[model_name][\"llm\"] = self.__get_custom_llm(model_name, access_token)\n",
    "            prompts[model_name][\"template\"] = self.__create_template(model_name)\n",
    "        print(\"\")\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def create_chain(self, retriever, model_name, template = None):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        if template is None:\n",
    "            template = prompt[\"template\"].get_template_for_use_retriever()\n",
    "        template = PromptTemplate.from_template(template)\n",
    "        chain = (\n",
    "            {\"context\": retriever | MyGenerator.__format_docs, \"question\": RunnablePassthrough()}\n",
    "            | template\n",
    "            | prompt[\"llm\"]\n",
    "        )\n",
    "        print(f\"{model_name=}\" + \"\\n\" + (\"-\" * 30))\n",
    "        print(f\"{chain=}\" + \"\\n\" + (\"-\" * 30))\n",
    "        return chain\n",
    "\n",
    "    def create_chain_not_retriever(self, model_name, template = None):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        if template is None:\n",
    "            template = prompt[\"template\"].get_template_for_not_retriever()\n",
    "        template = PromptTemplate.from_template(template)\n",
    "        chain = (\n",
    "            {\"question\": RunnablePassthrough()}\n",
    "            | template\n",
    "            | prompt[\"llm\"]\n",
    "        )\n",
    "        print(f\"{model_name=}\" + \"\\n\" + (\"-\" * 30))\n",
    "        print(f\"{chain=}\" + \"\\n\" + (\"-\" * 30))\n",
    "        return chain\n",
    "\n",
    "    def extract_answer_from_response(self, model_name, response):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        answer = prompt[\"template\"].extract_answer_from_response(response)\n",
    "        return answer\n",
    "\n",
    "    def make_template_for_conversation(self, model_name, conversation):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        addition = prompt[\"template\"].get_additional_template_for_conversation()\n",
    "        conversation = conversation.replace('{', '{{')\n",
    "        conversation = conversation.replace('}', '}}')\n",
    "        conversation += addition\n",
    "        return conversation\n",
    "\n",
    "    def get_template(self, model_name):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        template = prompt[\"template\"].get_template_for_use_retriever()\n",
    "        return template\n",
    "\n",
    "    def get_template_not_retriever(self, model_name):\n",
    "        prompt = self.__get_prompt(model_name)\n",
    "        template = prompt[\"template\"].get_template_for_not_retriever()\n",
    "        return template\n",
    "\n",
    "    def __get_prompt(self, model_name):\n",
    "        return self.prompts[model_name]\n",
    "\n",
    "    # Replace similarity informations retrieved from vector db into a placeholder of context.\n",
    "    @staticmethod\n",
    "    def __format_docs(docs):\n",
    "        # return \"* \" + \"\\n* \".join(doc.page_content for doc in docs)\n",
    "        index = 0\n",
    "        content = \"\"\n",
    "        for doc in docs:\n",
    "            index += 1\n",
    "            try:\n",
    "                jobj = json.loads(doc.page_content)\n",
    "                content += (\"- %d:\\n\" % (index))\n",
    "                for mykey in jobj.keys():\n",
    "                    content += (\"\\t- %s: %s\\n\" % (mykey, jobj[mykey]))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(sys.exc_info())\n",
    "                print(e)\n",
    "                content += (\"- %s\\n\" % (doc.page_content))\n",
    "        return content\n",
    "\n",
    "    def __get_custom_llm(self, trained_model_name, access_token):\n",
    "        my_method = inspect.currentframe().f_code.co_name\n",
    "        print(\">>> 1/4[%s]: model = AutoModelForCausalLM.from_pretrained()\" % (my_method))\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            trained_model_name,\n",
    "            device_map = \"auto\",\n",
    "            low_cpu_mem_usage = True,\n",
    "            torch_dtype = \"auto\",\n",
    "            trust_remote_code = True,\n",
    "            token = access_token,\n",
    "        )\n",
    "        print(\">>> 2/4[%s]: tokenizer = AutoTokenizer.from_pretrained()\" % (my_method))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            trained_model_name,\n",
    "            token = access_token\n",
    "        )\n",
    "        print(\">>> 3/4[%s]: pipe = pipeline()\" % (my_method))\n",
    "        pipe = pipeline(\n",
    "            'text-generation',\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens = 1024,\n",
    "            torch_dtype = \"auto\",\n",
    "        )\n",
    "        print(\">>> 4/4[%s]: llm = HuggingFacePipeline()\" % (my_method))\n",
    "        llm = HuggingFacePipeline(\n",
    "            pipeline=pipe\n",
    "        )\n",
    "        return llm\n",
    "\n",
    "    def __create_template(self, model_name):\n",
    "        template_obj = None;\n",
    "        if 'google/gemma' in model_name:\n",
    "            template_obj = MyTemplateImpl4Gemma()\n",
    "        elif 'microsoft/Phi' in model_name:\n",
    "            template_obj = MyTemplateImpl4MsPhi()\n",
    "        elif 'cyberagent/open-calm' in model_name:\n",
    "            template_obj = MyTemplateImpl4OpenCalm()\n",
    "        elif 'rinna/japanese-gpt' in model_name:\n",
    "            template_obj = MyTemplateImpl4RinnaJpGpt()\n",
    "        elif 'meta-llama/Llama' in model_name:\n",
    "            template_obj = MyTemplateImpl4Llama3()\n",
    "        elif '/DeepSeek' in model_name:\n",
    "            template_obj = MyTemplateImpl4DeepSeek()\n",
    "        return template_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccb844-96ef-4a6d-96e6-d6acf7b38aa9",
   "metadata": {},
   "source": [
    "### 【生成】Open LLM一覧\n",
    "取り扱うOpen LLM名の一覧を管理するオブジェクトを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d7ad9-3579-41ac-8b5d-6b888debf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "openllm_list = MyOpenLlmList([2])\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca351ba4-cc36-4b04-96f6-3dff2199fbb1",
   "metadata": {},
   "source": [
    "### 【生成】Generator Ojbect\n",
    "プロンプトを使ってLLMへ回答案の作成依頼を連携するGeneratorオプジェクトを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150c275-35da-4b11-b72b-0686f710d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "from mylib.MyGenerator import MyGenerator\n",
    "\n",
    "generator = MyGenerator(openllm_list.getAll(), HF_ACCESS_TOKEN)\n",
    "print(f\"{generator=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae468e6-93f6-4815-aa44-86d5000ff701",
   "metadata": {},
   "source": [
    "## 4. 生成: chain (=①+②)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54fa5e-b615-4869-9e75-f8e36c92b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "# chain作成\n",
    "chain = generator.create_chain(retriever, openllm_list.get(0))\n",
    "# chain = generator.create_chain_not_retriever(openllm_list.get(0))\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d884b-04c8-4dd1-ae88-97d1f35d88fd",
   "metadata": {},
   "source": [
    "## 5. 拡張検索(RAG)実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7a752-391d-4352-8dd9-bd1074154299",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "#query=\"Excelを使いこなしたい\"\n",
    "#query=\"AIに関するセッションの詳細を教えてください\"\n",
    "query = \"パソコンの使い方を学べるセミナーはありますか？\"\n",
    "\n",
    "print(f\"{chain.invoke(query)=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ffda5-3b48-439f-9107-4fdc75df5ded",
   "metadata": {},
   "source": [
    "## 6. 本ステップを終えて"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c3928-febf-4834-b3cc-f2abebadd2cf",
   "metadata": {},
   "source": [
    "ここまでの手順でRAGの一通りの実装を経験しました。次のステップではここまでに経験してきたナレッジを活用して、簡易的なRAGアプリケーションの構築を経験します。\n",
    "- 次のStep ≫ [Step 5: Web UI (Chatting with Open LLM)](./rag-step05-web_ui_to_chat_with_llm.ipynb)\n",
    "- 今のStep ≫ Step 4: Retriever+Generator構築\n",
    "- 前のStep ≫ [Step 3: LLM Template作成](./rag-step03-llm_template.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
