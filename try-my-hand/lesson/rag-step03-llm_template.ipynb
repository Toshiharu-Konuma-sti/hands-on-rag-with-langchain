{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b212451d-8f82-4df4-87e1-3e805e8baf88",
   "metadata": {},
   "source": [
    "# Step 3: LLM Template作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedd2bf-b1a3-4f8e-9375-61ebf5e66b30",
   "metadata": {},
   "source": [
    "本ステップでは、質問者から投げかけられたクエリーと類似検索で得られた類似情報を使って、LLMに回答案の作成を依頼するために必要なテンプレートを準備する過程を経験します。\n",
    "- 各Open LLM向けのテンプレート実装でクラス構造を統一するため、MyTemplateInterfaceインターフェースを用意します\n",
    "- MyTemplateInterfaceを継承して各Open LLMの仕様に基づき、それぞれのテンプレートをクラスで作成します\n",
    "- テンプレートにはOpen LLMごとに以下を実装します\n",
    "    - 類似検索を付与する新規質問\n",
    "    - 類似検索を使わない新規質問\n",
    "    - LLMの返答結果から回答を抽出する処理\n",
    "    - 追加質問\n",
    "- 実際にOpen LLMをメモリに読み込み、作成したテンプレートの出来具合を試します\n",
    "![Step3](../image/rag-overview-step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037633ff-e8c3-46b4-8434-12192caa5c51",
   "metadata": {},
   "source": [
    "## 0. 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358b8c3-def2-4dbd-917d-9c452ba1b989",
   "metadata": {},
   "source": [
    "### 共通処理/定数定義\n",
    "バナークラスを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82783e-726a-4400-8f1d-ef62816c40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mylib.MyBanner import MyBanner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c0a3f-e048-465d-b275-93d9b9bc343c",
   "metadata": {},
   "source": [
    "### Access Token入力\n",
    "Open LLMの取得に必要となるHugging FaceのAccess Tokenを入力します。（事前に発行して入手しておきます）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f113c3c-6c15-4937-be85-a5d4d5f1efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from getpass import getpass\n",
    "HF_ACCESS_TOKEN = getpass(\"Hugging Face の Access Token を入力して Enter Key を押してください: \")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48858688-1a4d-4ae1-bb64-13933cbdbe39",
   "metadata": {},
   "source": [
    "### パッケージインストール\n",
    "本ステップの処理で依存するパッケージをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213db21-85c6-43fc-a28a-caeaeaf1aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "!python -V\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install langchain\n",
    "!pip install langchain-huggingface\n",
    "\n",
    "!pip install ipywidgets\n",
    "!pip install urllib3==1.26.20\n",
    "\n",
    "!pip install optimum[openvino]\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f83a4a-74e6-4523-b8ed-94766c51a99c",
   "metadata": {},
   "source": [
    "### import\n",
    "本ステップの処理で依存するモジュールを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab7e8e-ba81-4945-a35e-f4c518a96f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, pipeline)\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import torch\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71407f-dec2-4f96-981c-cb9e03f4c1ea",
   "metadata": {},
   "source": [
    "## 1. テンプレート作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700a8a7-3d89-4eb2-894a-d296699959a0",
   "metadata": {},
   "source": [
    "### 【定義】MyTemplateInterface\n",
    "各Open LLM向けのテンプレート実装でクラス構造を統一するためインターフェースを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d20644-6b71-49c1-be22-8aac27b64f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateInterface.py\n",
    "import abc\n",
    "\n",
    "class MyTemplateInterface(metaclass=abc.ABCMeta):\n",
    "\n",
    "    test_context = \"\"\"- 1:\n",
    "\\t- id: 141737\n",
    "\\t- category: 経済産業・社会,対面講座\n",
    "\\t- date: 2025-03-03\n",
    "\\t- title: ビジネスソフト実践科\n",
    "\\t- url: https://www.recurrent-navi.metro.tokyo.lg.jp/course/141737\n",
    "\\t- summary: コースの内容\\\\nWordやExcelをMOS資格取得レベルまで、またPowerPointの基礎も習得し、業務で活かせるスキルを身に付けます。オリジナルの教材と市販の教材を使用して基礎からしっかりステップアップしていきます。さらに、実務を想定した演習問題で実践力を養います。各種試験会場になっているので、慣れた環境で効率よく受験が可能です。就職支援ではキャリアコンサルタントが常駐しているので、就職相談、応募書類の添削や模擬面接も個別で随時対応します。また、採用実績のある企業の説明会等を実施します。訓練初期から就職に向けての行動計画を立て、スキルを習得しつつ就職に向けて早期に行動するよう促します。駅からも学校からも近い託児施設と契約しているため、希望者の方（条件有り）は安心してお子様を預けて学習に専念することができます。\\n※本講座は3/3～5/30の期間を通じて行われます。\n",
    "\\t- 場所: 多摩・島しょ部,北多摩エリア,武蔵野市\n",
    "\\t- 主催者: 東京都（実施機関） 専門学校中野スクールオブビジネス\n",
    "\\t- 定員数: 30名\n",
    "\\t- 費用: 無料（別途、教科書代等は本人負担となります。）\n",
    "\\t- 申込期日: 2025年1月17日\n",
    "- 2:\n",
    "\\t- id: 83980\n",
    "\\t- category: 経済産業・社会,対面講座\n",
    "\\t- date: 2025-03-08\n",
    "\\t- title: WordPressによるWebサイト制作\n",
    "\\t- url: https://www.recurrent-navi.metro.tokyo.lg.jp/course/83980\n",
    "\\t- summary: WordPressの概要、システムの構成、作業の概要、WordPressの設定、テーマ(テンプレート)の作成、プラグインの作成\n",
    "\\t- 場所: 23区,北区,城北エリア\n",
    "\\t- 主催者: 中央・城北職業能力開発センター赤羽校\n",
    "\\t- 定員数: 29名\n",
    "\\t- 費用: 6,500円\n",
    "\\t- 申込期日: 2024年12月10日\n",
    "- 3:\n",
    "\\t- id: 111975\n",
    "\\t- category: 経済産業・社会,対面講座\n",
    "\\t- date: 2025-03-02\n",
    "\\t- title: ホームページビルダーによるホームページ作成\n",
    "\\t- url: https://www.recurrent-navi.metro.tokyo.lg.jp/course/111975\n",
    "\\t- summary: ホームページの基礎知識、Webサイトとトップページの作成および編集、リンクの設定、画像の作成と編集\\n【ホーページビルダー21】\n",
    "\\t- 場所: 多摩・島しょ部,北多摩エリア,府中市\n",
    "\\t- 主催者: 多摩職業能力開発センター府中校\n",
    "\\t- 定員数: 25名\n",
    "\\t- 費用: 6,500円\n",
    "\\t- 申込期日: 2025年1月10日\n",
    "- 4:\n",
    "\\t- id: 136726\n",
    "\\t- category: 経済産業・社会,オンライン講座,対面講座\n",
    "\\t- date: null\n",
    "\\t- title: 学校向け出前講座\n",
    "\\t- url: https://www.recurrent-navi.metro.tokyo.lg.jp/course/136726\n",
    "\\t- summary: 消費生活相談や商品テスト指導などの経験を積んだ東京都消費者啓発員（コンシューマー・エイド）が、悪質商法被害の実例に基づき、被害防止の方法・対策について、詳しく解説いたします。\\n・契約とは何か／成年年齢引き下げに伴う消費者としての心得\\n・悪質商法被害防止（マルチ商法・定期購入など）\\n・インターネットやSNSのトラブル防止\\n・お金の使い方（キャッシュレス、ローンやクレジットの仕組み）\\n・糖度の測定（実験講座）\n",
    "\\t- 場所: オンライン,23区,多摩・島しょ部\n",
    "\\t- 主催者: 申込者\n",
    "\\t- 定員数: 原則 10名以上\n",
    "\\t- 費用: 無料\n",
    "\\t- 申込期日: 遅くても希望日の1か月前\n",
    "\"\"\"\n",
    "\n",
    "    test_question = \"パソコンの使い方を学べるセミナーを教えてください\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_template_for_use_retriever(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def get_template_for_not_retriever(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def extract_answer_from_response(self, response):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def test_get_context(self):\n",
    "        context_str = MyTemplateInterface.test_context\n",
    "        context_str = context_str.replace('{', '{{')\n",
    "        context_str = context_str.replace('}', '}}')\n",
    "        return context_str\n",
    "\n",
    "    def test_get_question(self):\n",
    "        return MyTemplateInterface.test_question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592298b-6510-4b98-98b3-c67a62c45bee",
   "metadata": {},
   "source": [
    "### 【定義】LLM別のテンプレ実装\n",
    "テンプレートの実装をしていますが、完成度はまだまだ未熟です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923369dc-0b37-49b2-8e27-4dd42f41b410",
   "metadata": {},
   "source": [
    "#### Gemma by Google\n",
    "- https://ai.google.dev/gemma/docs/formatting?hl=ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aeb3c3-7e2e-417d-98db-d1fb64a976f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4Gemma.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4Gemma(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"<start_of_turn>system\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "context以下に箇条書きでお伝えする情報を使用してuserからの質問に回答してください。\n",
    "\n",
    "context:\n",
    "{context}<end_of_turn>\n",
    "<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"<start_of_turn>system\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "userからの質問に回答してください。<end_of_turn>\n",
    "<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*<start_of_turn>model\", \"\", response, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"<end_of_turn>\n",
    "<start_of_turn>user\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a62867-c5a5-46f5-b4d6-fe0562f16996",
   "metadata": {},
   "source": [
    "#### Llama by Meta\n",
    "- https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-2/\n",
    "- https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d729de-b178-4613-ae7b-8ba13c52318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4Llama3.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4Llama3(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "context以下に箇条書きでお伝えする情報を使用してuserからの質問に回答してください。\n",
    "\n",
    "context:\n",
    "{context}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "userからの質問に回答してください。<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*<|start_header_id|>assistant<|end_header_id|>\", \"\", response, flags=(re.DOTALL))\n",
    "#        answer = re.sub(\"\\|\\|\", \"\", answer, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54b0e6-d2f4-4b36-959e-fcece8c9046a",
   "metadata": {},
   "source": [
    "#### Phi by Microsoft\n",
    "- https://huggingface.co/microsoft/Phi-3-mini-128k-instruct#chat-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392842d-931b-4f02-90c2-8578a5e4c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4MsPhi.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4MsPhi(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"<|system|>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "context以下に箇条書きでお伝えする情報を使用してuserからの質問に回答してください。\n",
    "\n",
    "context:\n",
    "{context}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"<|system|>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "userからの質問に回答してください。<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*<|assistant|>\", \"\", response, flags=(re.DOTALL))\n",
    "#        answer = re.sub(\"^\\|\\|\", \"\", answer, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a7147-5216-40d6-9d4e-52791c9d1a57",
   "metadata": {},
   "source": [
    "#### OpenCALM by Cyberagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e79a6-3f78-4cd8-932e-6cae0a782753",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4OpenCalm.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4OpenCalm(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "以下の情報を参考に回答してください。\n",
    "{context}\n",
    "A: \"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "A: \"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*\\nA:\", \"\", response, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "A: \"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe38be6-6736-4e97-843c-50287a545ce6",
   "metadata": {},
   "source": [
    "#### 日本語GPT言語モデル by rinna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f991b69-863e-4ae9-a9ce-9192d4f5574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4RinnaJpGpt.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4RinnaJpGpt(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "以下の情報を参考に回答してください。\n",
    "{context}\n",
    "A: \"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "A: \"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*\\nA:\", \"\", response, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"Q: {question}\n",
    "A: \"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551c451-d7e4-48b8-8a80-c7c87215c62c",
   "metadata": {},
   "source": [
    "#### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700f82f-a77e-4e0c-aedb-ad85064a23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyTemplateImpl4DeepSeek.py\n",
    "from mylib.MyTemplateInterface import MyTemplateInterface\n",
    "import re\n",
    "\n",
    "class MyTemplateImpl4DeepSeek(MyTemplateInterface):\n",
    "\n",
    "    def get_template_for_use_retriever(self):\n",
    "        template = \"\"\"<｜begin▁of▁sentence｜>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "context以下に箇条書きでお伝えする情報を使用してuserからの質問に回答してください。\n",
    "\n",
    "context:\n",
    "{context}\n",
    "<｜User｜>{question}\n",
    "<｜Assistant｜>\"\"\"\n",
    "        return template\n",
    "        \n",
    "    def get_template_for_not_retriever(self):\n",
    "        template = \"\"\"<｜begin▁of▁sentence｜>\n",
    "あなたは親切で、礼儀正しく、誠実で優秀な日本人のアシスタントです。\n",
    "userからの質問に回答してください。\n",
    "<｜User｜>{question}\n",
    "<｜Assistant｜>\"\"\"\n",
    "        return template\n",
    "\n",
    "    def extract_answer_from_response(self, response):\n",
    "        answer = re.sub(\".*<｜Assistant｜><think>\", \"\", response, flags=(re.DOTALL))\n",
    "        answer = answer.strip()\n",
    "        return answer\n",
    "\n",
    "    def get_additional_template_for_conversation(self):\n",
    "        template = \"\"\"<｜User｜>{question}\n",
    "<｜Assistant｜>\"\"\"\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89c113-8e37-4bab-833e-f9cf86babf83",
   "metadata": {},
   "source": [
    "## 2. OpenLLM一覧作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629d30e-2600-4364-9aa6-8bbf50b87233",
   "metadata": {},
   "source": [
    "取り扱うOpen LLMの一覧を配列で列記します。<br>\n",
    "以下Open LLMの利用には利用規約へ同意が必要のため、事前に利用申請を済ませておきます。\n",
    "- [Google / Gemma](https://huggingface.co/google/gemma-2-2b-jpn-it)\n",
    "- [Meta / Llama](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "\n",
    "モデルのダウンロード先は以下になります。\n",
    "- モデルのダウンロードPATH: ```~/.cache/huggingface/hub/```\n",
    "- サイズ確認のコマンド: ```$ du -sh ~/.cache/huggingface/hub/*```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb6f49-f39b-4c6f-96eb-ad08619b16d7",
   "metadata": {},
   "source": [
    "### 【定義】MyOpenLlmList Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d6797-bb5b-497f-bd2a-cc73ede4a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mylib/MyOpenLlmList.py\n",
    "\n",
    "class MyOpenLlmList:\n",
    "\n",
    "    trained_models = [\n",
    "        'google/gemma-2b-it',   # 0\n",
    "        'google/gemma-2-2b-it',   # 1\n",
    "        'google/gemma-2-2b-jpn-it',   # 2\n",
    "        'meta-llama/Llama-3.2-1B',  # 3\n",
    "        'meta-llama/Llama-3.2-1B-Instruct', # 4\n",
    "        'microsoft/Phi-3-mini-4k-instruct', # 5\n",
    "        'microsoft/Phi-3-mini-128k-instruct', # 6\n",
    "        'cyberagent/open-calm-small', # 7\n",
    "        'cyberagent/open-calm-medium', # 8\n",
    "        'cyberagent/open-calm-large', # 9\n",
    "        'rinna/japanese-gpt2-medium', # 10\n",
    "        'rinna/japanese-gpt-1b', # 11\n",
    "        'rinna/japanese-gpt-neox-3.6b-instruction-sft-v2', # 12\n",
    "        'lightblue/DeepSeek-R1-Distill-Qwen-1.5B-Multilingual', #13\n",
    "    ]\n",
    "\n",
    "    def __init__(self, enables = []):\n",
    "        self.__printModelList(\"Initial list\", self.trained_models)\n",
    "        elements = self.trained_models\n",
    "        if (len(enables) > 0):\n",
    "            elements = [self.trained_models[i] for i in enables]\n",
    "        self.trained_models = elements\n",
    "        self.__printModelList(\"Enabled list\", self.trained_models)\n",
    "\n",
    "    def __printModelList(self, caption, models):\n",
    "        print(f\"{caption}:\")\n",
    "        for i, model in enumerate(models):\n",
    "            print(f\" {i}: {model}\")\n",
    "\n",
    "    def getAll(self):\n",
    "        return self.trained_models\n",
    "\n",
    "    def get(self, index):\n",
    "        return self.trained_models[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169378f8-083c-41df-9813-41fbcb0c55f6",
   "metadata": {},
   "source": [
    "## 3. テンプレート実装確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c276f-7eb3-4514-9076-b603e7673c7a",
   "metadata": {},
   "source": [
    "### OpenLLM一覧生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb57d1-d92e-48ce-b4b6-2093cfee0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from mylib.MyOpenLlmList import MyOpenLlmList\n",
    "\n",
    "openllm_list = MyOpenLlmList()\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21ea71-b906-42dd-8a42-06e850d69e30",
   "metadata": {},
   "source": [
    "### 対象のLLMとテンプレ選定\n",
    "テンプレートの実装を確認する対象Open LLMを一つ選定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c71fb-ea06-4b7b-92e9-2a47c04fab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "from mylib.MyTemplateImpl4Gemma import MyTemplateImpl4Gemma\n",
    "from mylib.MyTemplateImpl4MsPhi import MyTemplateImpl4MsPhi\n",
    "from mylib.MyTemplateImpl4OpenCalm import MyTemplateImpl4OpenCalm\n",
    "from mylib.MyTemplateImpl4RinnaJpGpt import MyTemplateImpl4RinnaJpGpt\n",
    "from mylib.MyTemplateImpl4Llama3 import MyTemplateImpl4Llama3\n",
    "from mylib.MyTemplateImpl4DeepSeek import MyTemplateImpl4DeepSeek\n",
    "\n",
    "# select a model\n",
    "model_name = openllm_list.get(2)\n",
    "print(f\"{model_name=}\")\n",
    "\n",
    "# a template will be selected\n",
    "template_obj = None\n",
    "if 'google/gemma' in model_name:\n",
    "    template_obj = MyTemplateImpl4Gemma()\n",
    "elif 'microsoft/Phi' in model_name:\n",
    "    template_obj = MyTemplateImpl4MsPhi()\n",
    "elif 'cyberagent/open-calm' in model_name:\n",
    "    template_obj = MyTemplateImpl4OpenCalm()\n",
    "elif 'rinna/japanese-gpt' in model_name:\n",
    "    template_obj = MyTemplateImpl4RinnaJpGpt()\n",
    "elif 'meta-llama/Llama-3' in model_name:\n",
    "    template_obj = MyTemplateImpl4Llama3()\n",
    "elif '/DeepSeek' in model_name:\n",
    "    template_obj = MyTemplateImpl4DeepSeek()\n",
    "print(f\"{template_obj=}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "template_str = template_obj.get_template_for_use_retriever()\n",
    "#template_str = template_obj.get_template_for_not_retriever()\n",
    "print(f\"{template_str=}\")\n",
    "if '{context}' in template_str:\n",
    "    context_str = template_obj.test_get_context()\n",
    "    template_str = template_str.replace('{context}', context_str)\n",
    "print(f\"{template_str=}\")\n",
    "\n",
    "question = template_obj.test_get_question()\n",
    "print(f\"{question=}\")\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734cf85-f594-4078-821e-42f9603f00a1",
   "metadata": {},
   "source": [
    "### テンプレの実装確認\n",
    "テンプレートの実装を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa1644-c07d-4960-b025-88849db6218b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MyBanner.start()\n",
    "\n",
    "MyBanner.passing(\"01/10: start: check the environment for Intel hardware or other hardware\")\n",
    "IS_AVAILABLE_OVMODEL = False\n",
    "if torch.backends.mps.is_available() or torch.cuda.is_available():\n",
    "    print(\"- MPS or CUDA is available.\")\n",
    "else:\n",
    "    try:\n",
    "        from optimum.intel import OVModelForCausalLM\n",
    "        IS_AVAILABLE_OVMODEL = True\n",
    "        print(\"- OpenVINO (optimum-intel) is available.\")\n",
    "    except ImportError:\n",
    "        print(\"- OpenVINO (optimum-intel) not found. Will use PyTorch (AutoModelForCausalLM).\")\n",
    "\n",
    "MyBanner.passing(\"02/10: start: it will create model.\")\n",
    "if IS_AVAILABLE_OVMODEL:\n",
    "    print(\"- It will use OVModelForCausalLM.from_pretrained().\")\n",
    "    model = OVModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        export = True,\n",
    "        device = \"auto\",\n",
    "        token = HF_ACCESS_TOKEN,\n",
    "    )\n",
    "    print(\"- model was created by OVModelForCausalLM.from_pretrained().\")\n",
    "else:\n",
    "    print(\"- It will use AutoModelForCausalLM.from_pretrained().\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map = \"auto\",\n",
    "        low_cpu_mem_usage = True,\n",
    "        torch_dtype = \"auto\",\n",
    "        trust_remote_code = True,\n",
    "        token = HF_ACCESS_TOKEN,\n",
    "    )\n",
    "    print(\"- model was created by AutoModelForCausalLM.from_pretrained().\")\n",
    "\n",
    "print(f\"{model.device=}\")\n",
    "\n",
    "MyBanner.passing(\"03/10: start: tokenizer = AutoTokenizer.from_pretrained()\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = HF_ACCESS_TOKEN)\n",
    "\n",
    "MyBanner.passing(\"04/10: start: pipe = pipeline()\")\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 1024,\n",
    "    torch_dtype = \"auto\",\n",
    ")\n",
    "\n",
    "MyBanner.passing(\"05/10: start: llm = HuggingFacePipeline()\")\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")\n",
    "\n",
    "MyBanner.passing(\"06/10: start: template = PromptTemplate.from_template()\")\n",
    "template = PromptTemplate.from_template(template_str)\n",
    "\n",
    "MyBanner.passing(\"07/10: start: create chain\")\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | template\n",
    "    | llm\n",
    ")\n",
    "print(f\"{chain=}\")\n",
    "\n",
    "MyBanner.passing(\"08/10: start: chain.invoke(question)\")\n",
    "response = chain.invoke(question)\n",
    "print(f\"{response=}\")\n",
    "\n",
    "MyBanner.passing(\"09/10: start: extract an answer from response\")\n",
    "answer = template_obj.extract_answer_from_response(response)\n",
    "print(f\"{answer=}\")\n",
    "\n",
    "MyBanner.passing(\"10/10: start: display the first few characters of the answer in hex\")\n",
    "ascii_list = list(answer[:10])\n",
    "hex_list=[f'0x{ord(i):02X}' for i in ascii_list]\n",
    "hex_string=' '.join(hex_list)\n",
    "print(f'# {hex_string=}')\n",
    "\n",
    "MyBanner.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd9e5b-585a-44d4-a3e8-54151ac5054d",
   "metadata": {},
   "source": [
    "## 3. 本ステップを終えて"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967da32-e52d-4f8f-9681-564f48a4c0f6",
   "metadata": {},
   "source": [
    "ここまでの手順でLLMに回答案の生成を依頼するためのテンプレートを作成する過程を経験しました。次のステップではここまでに経験してきた類似検索と準備したテンプレートを活用してRetrieverとGeneratorの構築を経験します。\n",
    "- 次のStep ≫ [Step 4: Retriever+Generator構築](./rag-step04-retriever_and_generator.ipynb)\n",
    "- 今のStep ≫ Step 3: LLM Template作成\n",
    "- 前のStep ≫ [Step 2: Vector DBで類似検索](./rag-step02-search_from_vectordb.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
